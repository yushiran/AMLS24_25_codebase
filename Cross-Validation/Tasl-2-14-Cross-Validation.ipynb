{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.14: Regression\n",
    "## Ridge Regression with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to  tune the hyper-parameter associated with ridge regression using cross-validation procedures. You will be provided with some already complete code as well as some code that you should complete yourself. In particular, you will have to:\n",
    "\n",
    "* Complete the function `ridgeRegrCV(X_train, y_train, alpha,X_test)` that will make use of the pre-built implementation of Ridge regression with cross validation to select the best L2-penalty. Experiment with different cross validation methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "The required libraries for this notebook are pandas, sklearn, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston, load_iris\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ridge, RidgeCV\n",
      "File \u001b[0;32m/home/yushiran/anaconda3/envs/amls/lib/python3.11/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import *\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "The data we are using is from Boston house-prices dataset, which we can access directly from the scikit learn library . It consists of 506 data points and 13 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add another column that contains the house prices which in scikit learn datasets are considered as target\n",
    "boston=load_boston() # get the data\n",
    "#print(boston.keys()) # boston variable itself is a dictionary, so we can check for its keys \n",
    "print(boston.data.shape) # shape of data\n",
    "#print(boston.feature_names)\n",
    "boston_df=pd.DataFrame(boston.data,columns=boston.feature_names) # convert the boston.data to a a dataframe\n",
    "boston_df['Price']=boston.target # there is no column called ‘PRICE’ in the data frame because the target column is available in another attribute called target\n",
    "newX=boston_df.drop('Price',axis=1) # All other features\n",
    "newY=boston_df['Price'] # Boston Housing Price\n",
    "boston_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3) \n",
    "#test_size= should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split\n",
    "#everytime you run it without specifying random_state, you will get a different result, this is expected behavior\n",
    "#print (len(X_test), len(y_test))\n",
    "\n",
    "print('train set: {}  | test set: {}'.format(round(len(y_train)/len(newX),2),\n",
    "                                                       round(len(y_test)/len(newX),2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cross-validation for selecting the ridge regressor hyper-parameter\n",
    "\n",
    "`Ridge()` function from scikit-learn library has a hyper-parameter **alpha**  (we also refer to this hyper-parameter as λ) that is used to tune the model. Instead of choosing alpha manually, it is better to use cross-validation to optimize the selection of the hyper-parameter.\n",
    "\n",
    "Each alpha corresponds to a vector of thirteen ridge regression coefficients. We will create a 13×5 matrix to store the various combinations of hyper parameters and associated ridge regression coefficients (13 rows for our 13 predictors and 5 columns for our 5 generated alphas). You can of course change the number of generated alphas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate a range of alpha values and put them in a numpy array\n",
    "#r_alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "r_alphas = [0.001, 0.01, 0.1, 1, 10]\n",
    "#print(r_alphas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    " Make use of `RidgeCV()`, which is the cross-validated `Ridge()` function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ridgeRegrCVPredict(X_train, y_train, r_alphas,X_test):\n",
    "\n",
    "    ridgecv = RidgeCV(alphas = r_alphas, fit_intercept=True)\n",
    "\n",
    "    # Next step: fit ridgecv!\n",
    "    #print('Best alpha value: '+str(ridgecv.alpha_))\n",
    "\n",
    "    #Y_pred_cv = ...\n",
    "    return Y_pred_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Test your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_pred_cv =  ridgeRegrCVPredict(X_train, y_train, r_alphas,X_test)\n",
    "mse_cv=mean_squared_error(y_test,Y_pred_cv)\n",
    "print('Mean Squared Error (MSE) on test set (built-in cross-validation): '+str(mse_cv))\n",
    "\n",
    "plt.scatter(y_test, Y_pred_cv)\n",
    "plt.xlabel(\"Actual prices: $Y_i$\")\n",
    "plt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
    "plt.title(\"Actual prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
